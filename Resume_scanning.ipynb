{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKvKLTlpaSek"
      },
      "source": [
        "### Ref\n",
        "\n",
        "https://www.bluetickconsultants.com/resume-parsing-using-nlp.html\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/named-entity-recognition-and-classification-with-scikit-learn-f05372f07ba2\n",
        "\n",
        "\n",
        "https://blog.apilayer.com/build-your-own-resume-parser-using-python-and-nlp/ ",
        "\n",
        "\n",
        "https://youtu.be/4ssigWmExak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zmTdr9wza1r"
      },
      "source": [
        "## Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIEl8XAjVWui",
        "outputId": "265d44c0-1b7b-442c-a43a-b540615749de"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/machine-learning-playground/resume-scanner/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huj9trDuswQw"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOHtiJfQsazU"
      },
      "source": [
        "### flatten list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txLGhbdgsc6s"
      },
      "outputs": [],
      "source": [
        "def flatten(l):\n",
        "    return [item for sublist in l for item in sublist]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXc6b9E2NFbn"
      },
      "source": [
        "## Reading Resume\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shpKdLPCyR22"
      },
      "source": [
        "### Installing pdfminer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdRJnc8tMpUb",
        "outputId": "f8695b23-3d7f-444b-8bdc-1b59ed042258"
      },
      "outputs": [],
      "source": [
        "!pip install pdfminer.six\n",
        "!pip install nltk\n",
        "!pip install numpy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1mZgZbgWLBq",
        "outputId": "04303421-01fd-45d5-efd6-027b50530426"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def extract_names(txt):\n",
        "    person_names = []\n",
        " \n",
        "    for sent in nltk.sent_tokenize(txt):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "                person_names.append(\n",
        "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
        "                )\n",
        " \n",
        "    return person_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQudNtz-Vb4J"
      },
      "source": [
        "### Scanning the pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2hzJO3EVeV3"
      },
      "outputs": [],
      "source": [
        "# example_01.py\n",
        " \n",
        "from pdfminer.high_level import extract_text\n",
        " \n",
        " \n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhlxRK69XQhg"
      },
      "source": [
        "### Extracting phone numbers from resumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzjKVfoWXSEh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
        "\n",
        "def extract_phone_number(resume_text):\n",
        "    phone = re.findall(PHONE_REG, resume_text)\n",
        " \n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        " \n",
        "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
        "            return number\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viKNRlEWX3rP"
      },
      "source": [
        "### Extracting email addresses from resumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE435PZxX6UQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
        "\n",
        "def extract_emails(resume_text):\n",
        "    return re.findall(EMAIL_REG, resume_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vju6QJOYWJKd"
      },
      "source": [
        "### Extracting Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWorRZ92w7uF"
      },
      "outputs": [],
      "source": [
        "def extract_names(txt):\n",
        "    person_names = []\n",
        " \n",
        "    for sent in nltk.sent_tokenize(txt):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "                person_names.append(\n",
        "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
        "                )\n",
        " \n",
        "    return person_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDOxSzNgYQiB"
      },
      "source": [
        "### Extracting skills from the resumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGKGxTz9YTBs",
        "outputId": "52591a72-6109-42c9-8204-7860957dae27"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# TODO: read from csv\n",
        "SKILLS_DB = [\n",
        "    'php',\n",
        "    'wordpress',\n",
        "    'html',\n",
        "    'javascript',\n",
        "    'laravel',\n",
        "    'css',\n",
        "    'js',\n",
        "    'symfony',\n",
        "    'git',\n",
        "    'yii',\n",
        "    'aws',\n",
        "    'kafka',\n",
        "    'cakephp',\n",
        "    'react',\n",
        "    'vue',\n",
        "    'node',\n",
        "    'npm',\n",
        "    'composer',\n",
        "    'codeigniter',\n",
        "    'drupal',\n",
        "    'magento'\n",
        "]\n",
        "\n",
        "def extract_skills(input_text):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
        " \n",
        "    # remove the stop words\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
        " \n",
        "    # remove the punctuation\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
        " \n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
        " \n",
        "    # we create a set to keep the results in.\n",
        "    found_skills = set()\n",
        " \n",
        "    # we search for each token in our skills database\n",
        "    for token in filtered_tokens:\n",
        "        if token.lower() in SKILLS_DB:\n",
        "            found_skills.add(token)\n",
        " \n",
        "    # we search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in SKILLS_DB:\n",
        "            found_skills.add(ngram)\n",
        " \n",
        "    return found_skills"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MVPujM5Y7ux"
      },
      "source": [
        "### Extracting education and schools from resumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk3Yj6iMY8hy",
        "outputId": "892e1afc-2912-474e-b83d-9e336fb8fb5a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        " \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "RESERVED_WORDS_EDU = [\n",
        "    'school',\n",
        "    'college',\n",
        "    'science',\n",
        "    'engineering',\n",
        "    'bachelor',\n",
        "    'master',\n",
        "    'univers',\n",
        "    'academy',\n",
        "    'faculty',\n",
        "    'institute',\n",
        "]\n",
        "\n",
        "def extract_education(input_text):\n",
        "    organizations = []\n",
        " \n",
        "    # first get all the organization names using nltk\n",
        "    for sent in nltk.sent_tokenize(input_text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
        "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
        " \n",
        "    # we search for each bigram and trigram for reserved words\n",
        "    # (college, university etc...)\n",
        "    education = set()\n",
        "    for org in organizations:\n",
        "        for word in RESERVED_WORDS_EDU:\n",
        "            if org.lower().find(word) >= 0:\n",
        "                education.add(org)\n",
        " \n",
        "    return education"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc3r8TPzIzVP"
      },
      "source": [
        "### Extract Work Ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWyEZwmpI1hj",
        "outputId": "23c53ad0-8dc6-4a80-bc38-17196f3a45c9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        " \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "RESERVED_WORDS_WORK = [\n",
        "    'pvt',\n",
        "    'consultant',\n",
        "    'solutions',\n",
        "    'limited',\n",
        "    'technologies',\n",
        "    'indust'\n",
        "\n",
        "]\n",
        "\n",
        "def extract_workex(input_text):\n",
        "    organizations = []\n",
        " \n",
        "    # first get all the organization names using nltk\n",
        "    for sent in nltk.sent_tokenize(input_text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
        "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
        "\n",
        "    # we search for each bigram and trigram for reserved words\n",
        "    # (college, university etc...)\n",
        "    workex = set()\n",
        "    for org in organizations:\n",
        "        for word in RESERVED_WORDS_WORK:\n",
        "            if org.lower().find(word) >= 0:\n",
        "                workex.add(org)\n",
        " \n",
        "    return workex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W2khgCTWTzb"
      },
      "source": [
        "### Driver Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbMJMXX2WWEf",
        "outputId": "ad6b1c6e-cefa-414c-d24b-3fde2d0614fc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "resumeFolder = DRIVE_PATH+'resume/'\n",
        "filePath = resumeFolder+sorted(os.listdir(resumeFolder))[0]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    text = extract_text_from_pdf(filePath)\n",
        "    names = extract_names(text)\n",
        "    print(names)\n",
        "    phone_number = extract_phone_number(text)\n",
        "    print(phone_number)\n",
        "    emails = extract_emails(text)\n",
        "    print(emails)\n",
        "    resumeSkillsList = extract_skills(text)\n",
        "    skills = ', '.join(str(e) for e in resumeSkillsList)\n",
        "    print(skills)\n",
        "    education_information = extract_education(text)\n",
        "    print(education_information)\n",
        "    workex = extract_workex(text)\n",
        "    print(workex)\n",
        "    intro = names[0] + \"\\n\" + emails[0] + \"\\n\" + phone_number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5p-DvN30Tzp"
      },
      "source": [
        "## Detect Keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TZ8Fooz0ZY8"
      },
      "source": [
        "### skills driver method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXtPpUu235Ns"
      },
      "outputs": [],
      "source": [
        "def detectSkills(skillName, resumeSkillList):\n",
        "  for skill in resumeSkillList:\n",
        "    if skillName.lower() in skill.lower():\n",
        "      return \"Yes\"\n",
        "  return \"No\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6HVAQIn1EW4"
      },
      "source": [
        "### Check Skills"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc0bWe0i1Gk-",
        "outputId": "f0a156ac-8b06-4dfe-facf-b09006f6b4ce"
      },
      "outputs": [],
      "source": [
        "print(resumeSkillsList)\n",
        "\n",
        "skillList = {\n",
        "  \"javascript\": [\n",
        "      \"javascript\"\n",
        "  ],\n",
        "  'javascript-framework':[\n",
        "      'react',\n",
        "      'vue',\n",
        "      'node',\n",
        "      'express'\n",
        "  ],\n",
        "  \"php\": [\n",
        "      \"php\"\n",
        "  ],\n",
        "  \"wordpress\":[\n",
        "      \"wordpress\"\n",
        "  ],\n",
        "  \"php-framework\": [\n",
        "      \"laravel\",\n",
        "      \"yii\",\n",
        "      \"symphony\",\n",
        "      \"codeigniter\"\n",
        "  ],\n",
        "  \"css\": [\n",
        "      \"sass\",\n",
        "      \"scss\",\n",
        "      \"tailwind\",\n",
        "      \"css\"\n",
        "  ],\n",
        "  \"html\": [\n",
        "      \"html5\",\n",
        "      \"html\"\n",
        "  ]\n",
        "}\n",
        "\n",
        "verdict = {}\n",
        "\n",
        "for skillName, skillKeywords in skillList.items():\n",
        "  for skillKeyword in skillKeywords:\n",
        "    doesExists = detectSkills(skillKeyword, resumeSkillsList)\n",
        "    if (\"Yes\" == doesExists):\n",
        "      verdict[skillName] = \"Yes\"\n",
        "      continue\n",
        "    verdict[skillName] = \"No\"\n",
        "print(verdict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGMsJBd1ZcUm"
      },
      "source": [
        "## Updating value in a csv standard format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnJY_XvAhbGA"
      },
      "source": [
        "### Installing Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6WT9vjRhd1G"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-8xm1CjgUwW"
      },
      "source": [
        "### Adding sheets API dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-xcmdamZg26"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os.path\n",
        "from google.oauth2.credentials import Credentials\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from google.oauth2 import service_account"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5iUk3T1xGT9"
      },
      "source": [
        "### Creds for sheets API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqMX36GpxIdw"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "COLUMNS = list(string.ascii_uppercase)\n",
        "INITIAL_ROW = \"1\"\n",
        "SCORING_TAB_NAME = \"automated_scoring\"\n",
        "KEYWORD_TAB_NAME = \"automated_keywords\"\n",
        "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
        "SERVICE_ACCOUNT_FILE = '/content/drive/MyDrive/machine-learning-playground/resume-scanner/resume-scanner-361910-4803b5ebb28d.json'\n",
        "SPREADSHEET_ID = '1pdt2be42qZI0YmjDlke4FbTDQZt244HFtJV_D-SgolY'\n",
        "\n",
        "creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "\n",
        "service = build('sheets', 'v4', credentials=creds)\n",
        "sheet = service.spreadsheets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O6Ky25Sghtr"
      },
      "source": [
        "### Reading values from sheet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9VFhKMLgkyD"
      },
      "outputs": [],
      "source": [
        "def readSheet(spreadsheet_id, range_name):\n",
        "    try:\n",
        "        result = sheet.values().get(spreadsheetId=SPREADSHEET_ID, range=range_name).execute()\n",
        "        values = result.get('values', [])\n",
        "        if not values:\n",
        "            print('No data found.')\n",
        "            return\n",
        "        else:\n",
        "            return values\n",
        "    except HttpError as err:\n",
        "        print(err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbPPnCqXRJrC"
      },
      "source": [
        "### Write Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMhyr5OkmfLD"
      },
      "outputs": [],
      "source": [
        "def writeDate(spreadsheet_id, range_name, data):\n",
        "  requestBody = {\n",
        "      \"values\": data\n",
        "  }\n",
        "  request = sheet.values().update(spreadsheetId=spreadsheet_id, range=range_name, valueInputOption=\"USER_ENTERED\", body=requestBody)\n",
        "  response = request.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INLeTVEggmBR"
      },
      "source": [
        "### Driver Code\n",
        "\n",
        "- check which column in row 1 is empty\n",
        "- if no column found add 10 columns\n",
        "- write data in that column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejW88PrygoiX",
        "outputId": "eb710b72-46bc-4133-ef42-6f9c701ef294"
      },
      "outputs": [],
      "source": [
        "row1 = flatten(readSheet(SPREADSHEET_ID, \"automated_scoring!1:1\"))\n",
        "starting_cell = COLUMNS[len(row1)]+INITIAL_ROW\n",
        "print(starting_cell)\n",
        "# data = [names[0]+'\\n'+emails+'\\n'+phone_number, '', '', '', '', skills]\n",
        "data = [[intro],['Yes'],['Yes'],['Yes'],['Yes']]\n",
        "write_range = SCORING_TAB_NAME+\"!\"+starting_cell\n",
        "print(write_range)\n",
        "values = writeDate(SPREADSHEET_ID, write_range, data)\n",
        "print(values)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Huj9trDuswQw",
        "shpKdLPCyR22",
        "SQudNtz-Vb4J",
        "vhlxRK69XQhg",
        "viKNRlEWX3rP",
        "6MVPujM5Y7ux",
        "Vc3r8TPzIzVP",
        "cGMsJBd1ZcUm",
        "GnJY_XvAhbGA",
        "m-8xm1CjgUwW"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
